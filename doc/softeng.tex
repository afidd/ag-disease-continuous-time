\documentclass{article}
\usepackage{times}
\usepackage{mathptmx}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}\usepackage{xspace}

\title{Technical Document: Software Engineering Challenges for Stochastic Simulation}
\author{Drew Dolgert, Chris Myers, Dave Schneider}
\date{\today}
\begin{document}
\maketitle

\newcommand{\naadsm}{\textsc{naadsm}\xspace}
\newcommand{\adct}{\textsc{adct}\xspace}
\newcommand{\hpai}{\textsc{hpai}\xspace}
\newcommand{\fmd}{\textsc{fmd}\xspace}
\newcommand{\gui}{\textsc{gui}\xspace}
\newcommand{\nsc}{\textsc{sc}\xspace}
\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}\xspace}

\tableofcontents

\section{Introduction}
The North American Animal Disease Simulation Model (\naadsm)
simulates spread of disease among agricultural units on a
landscape and interventions to the spread of disease\cite{naadsm}.
Analysts at the regulatory agency \textsc{usda--aphis} use
these simulations to understand how policies affect
risk of disease spread. The \naadsm software is apt for this
task in that it has a graphical user interface for modifying
scenario parameters and analyzing summary results of dozens of
runs of the simulation.

This document asks, instead, how \naadsm in particular,
or any similar simulation, might be modified for more
quantitative use. 
The model simulated by \naadsm, separate from its user
interface, is a dynamical simulation of disease which
could serve statistical risk analysis.
Given spread of an outbreak by unknown processes, a statistical
technique called \emph{model selection} can use \naadsm
to infer parameters for different processes and then estimate
the likelihood of possible processes according to \emph{model
information criteria}. Given an outbreak of a known disease,
the statistical technique of \emph{estimation} can use thousands
of runs of \naadsm to estimate effectiveness of interventions.
Both of these quantitative uses of \naadsm require that statistical
algorithms choose parameters for \naadsm, run it many times, and
manipulate the outcomes of those runs. \naadsm must be a cog
in a larger machine, so its architecture must serve other software.



Whether software runs correctly is a perennial problem\cite{hoare1996}.
The functioning of \naadsm, in particular, determines behavior
and expenditures for both agricultural operators and the \textsc{usda}.
\naadsm simulates disease spread, disease spread rate determines
risk, and risk determines governmental policy for intervention
and mitigation of economic and animal harm.
Given a scenario for introduction of disease, \naadsm produces
many possible trajectories for spread across a landscape, so the
results are uncertain, but this doesn't make precision in calculation
less important. There is always a threshold of risk at which
governmental policy assigns quarantine, depopulation or movement
restrictions, so an erroneous shift in the uncertain predictions
could result in farms or regions of farms either overly at risk
or overly risk-managed. Both are costly.

\naadsm is also uncommon in that it is both economically important
and a public instrument of the government, so that it has already
supported external validation, extensive testing, and the
benefit of many eyes on the code. It has traits of excellent
software.
\begin{itemize}
  \item The code is written to be read, as described
      in \emph{Code Complete}\cite{mcconnell2004code}. Organization
      of code and naming contribute to review of code.
  \item Best-in-class libraries are used where possible. These
    include the Gnu Scientific Libraries for distributions
    and the Scalable Parallel Pseudo Random Number Generators
    Library for random numbers.
  \item Different processes for spread of disease are separated
    into different ``models'' within the code. A responsible party,
    both programmer and veterinarian, has signed each model.
  \item The DejaGnu software performs unit tests at the model
    level. These unit tests are simple to run and organized.
  \item There is an open specification for each scientific
    model. The specification and code read like a diptych,
    one parallel to the other.
  \item Calculation of disease processes is completely separate
    from graphical display of either choices about a scenario
    or results of a scenario. Each is written in a language
    appropriate to the task, \CC and Delphi, respectively.
  \item The use of version control, assertions, and clear
    comments follow best practices\cite{hunt2000pragmatic}.
\end{itemize}
This is a code for which a governing body has allocated resources
appropriate for the importance of the tool, and the resulting
simulation has earned the trust of analysts who estimate risk.

It may therefore be surprising that the following sections
describe unintended behavior in a simulation model,
failure to follow specifications, and even failure to
correctly sample a random number. None of this is
surprising to the authors who are responsible, themselves,
for constructing simulation codes. In the absence of formal
techniques from mathematics and computer science, any code
of this size and complexity will have what programmers
call ``defects.''

This document asks, therefore, what are desirable traits
of scientific software as important as \naadsm and what techniques
guard against defects that might affect risk analysis and subsequent
policy decisions? The document looks at \naadsm 3.2.19, which
is an older, well-known version. The lessons below come from
developers who have a lot of experience and may occasionally
still be stupid. The core of the challenge is an effort
to perform scientific analysis of a complicated simulation,
where that analysis is beyond the user requirements for
the application.


\section{The Scientific Interface}
This section looks at the steps required to treat the
underlying model of \naadsm as a piece of a larger
statistical calculation. The points made in this section
are prosaic, but the goal is for the hard work of
creating and validating these models to be available
for novel analysis from tools like \textsc{r} or \textsc{sas}.


\subsection{Building the Simulation}
\naadsm 3.2.19 is available as source code on a public web site.
Building the application is, of necessity, complex. It is meant
to run on both Windows and Linux. There are about a dozen
supporting libraries to install. \naadsm, itself, requires about
a hundred thousand lines of code just for the core simulation.

The specific problems building \naadsm 3.2.19 are the following.
\begin{itemize}
  \item The library \texttt{gd} did not have a makefile or
        any standard installation instructions.
  \item The \texttt{configure} script included options to specify
        installation locations of some, but not all, supporting
        libraries. For those libraries, they just had to be moved
        somewhere the \texttt{configure} script would already look.
        The \texttt{rtree} library is an example of this.
  \item Two source files were required to compile but not included
        in the \texttt{configure} script. These were 
        \texttt{herd-randomizer.c} and \texttt{herd-randomizer.h}.
  \item The \texttt{configure} script uses a version of autotools
        which is more than a few years old. The script needed to
        be updated to work with available versions.
        Among the updates were quoting of arguments to macros
        and setting an \texttt{ACLOCAL\_PATH} variable to
        permit the \texttt{aclocal} autotool to find macros.
\end{itemize}
One consequence is that building \naadsm
on Linux required more than a day of effort. Worse is that
the absence of \texttt{herd-randomizer.c} suggests the final build
of version 3.2.19 presented on the web may differ from the
configuration file used to build the delivered code. Version 3.2.19
was extensively validated, but which files were in the version
that was validated?

The two ways to address these problems are to automate a build
of the code and to provide a prebuilt build environment.

Automation of builds is associated with continuous integration.
The practice of continuous integration requires that, after every
check-in to the versioning repository, the development team
halts production to address errors. This practice may not help
most teams and doesn't address the problems cited above. More
interesting is the technology used to perform continuous
integration. A task server, such as Hudson or Jenkins,
can, in concert with other tools, create a virtual machine,
fetch source code and supporting libraries, compile all of it,
and test the result, all at the press of a button. This capability
offers reassurance that code will compile on multiple operating
systems with multiple versions of libraries and would surely
avert the problems seen above.

\naadsm is distributed as a prebuilt application for Windows
and as source code. A possible further step of reliability would
be to distribute either a virtual machine (such as VirtualBox)
or a containers virtualization (such as OpenVZ) which runs
\textsc{naadsm/pc} and \textsc{naadsm/sc}.
This would address a few possible challenges. Windows programs
sometimes stop working after a few years thanks to changes to
the operating system or compatibility with supporting libraries.
Distributing an application with operating system, as
a virtual machine, increases
longevity of an application as a scientific reference.
In the case that deployment of simulations to a supercomputer
is required, containers virtualization would be very handy
and efficient way to run a reference copy of the application
with little overhead.


\subsection{Developer Documentation}
\naadsm has excellent documentation for its \textsc{pc} version,
the one with the \textsc{gui}, but there is little documentation
for users of the \textsc{sc} version or for authors of code.
This matters for three cases.

\subsection{Testing the Simulation Model}
The application's primary output is a trajectory for the
scenario. It's a list of states over time. The specification
for the simulation is sufficiently complex that researchers
spend time asking both why it exhibits certain behaviors
and how it made those choices. Examination of the internal
state of the application during a time step helps answer those
questions. Other scientists, and possibly analysts, will want
to know more than the basic set of states.

\naadsm delivers lots of information for every run and every time
step in four different ways. It's an unusually thorough effort
and yet difficult to use.

As an example, which infected unit infected the given
susceptible unit during the last time step? Did the
infection occur by airborne spread or direct contact?


\subsubsection{Scenario Monitor Output}
Every \naadsm scenario creates what are called models within
the application, and some of those models are of a sort
called \emph{monitors}. Each monitor is responsible
for reporting some portion of the state of the program.
A binary file encodes the scenario for the \textsc{gui}
version of \naadsm, and a \textsc{utf--16} \textsc{xml}
file encodes the scenario for the \textsc{sc} version
of \naadsm. Entries in these files specify monitors
for a scenario.

While those entries create the monitor object within
the running simulation, they do not appear to turn on
output to file or screen by that monitor. This requires,
within the \textsc{gui}, checking a separate box.
It's unclear to the authors how to enable printout for
the \textsc{sc} version.

Output from the monitors isn't documented, either,
and appears to be in a format intended neither for visual
inspection nor for computer parsing. It isn't \textsc{csv},
\textsc{xml}, or \textsc{json}.


\subsubsection{NAADSM/GUI Data Output}
The \naadsm Windows application offers many charts, graphs,
and tables.
If the goal is quantitative analysis of sets of runs,
then we will want to look at tables of data from runs in Excel,
R, or something similar.
The \textsc{gui} will save separate histograms of susceptible,
infected, or recovered, averaged over several runs.
There is a technical, statistical problem with this output
for upstream analysis because statistical estimation
taken from runs averages over the properties of each run,
on a run-by-run basis. In other words, all the traits of
a single run need to be grouped together in order to
perform estimation.
The \textsc{gui} does also write files with data for
each run. These files come from the monitors and, once
interpreted, would likely show the requisite per-run information,
but this is the route an analyst would use for quantitative
analysis.


\subsubsection{NAADSM/SC Tracing}
Tracing is a technique to report values as they are
computed in the code. Tracing libraries often write to
the screen on the standard error channel or write to
files, but they may also write to system services or
network monitors. Tracing is the most basic form of
debugging code.

We had a question about what values were used to weight
the exponential airborne model, to account for 
size of units.
\naadsm has a tracing facility, called with the
\texttt{g\_log} command. The tracing facility would
have been an excellent way to find this information,
but it wasn't clear how to enable it.

There are standard logging facilities for most languages,
almost all mirroring the capabilities of Java's log4j.
They offer great features, foremost of which is having
separate documentation on how to enable and use them.


\subsubsection{NAADSM/GUI Database Queries}
The Windows application version of \naadsm stores data
from runs in a database suitable for Microsoft Access.
It's a great idea, an appropriate tool for the task.
The user interface offers a generalized \textsc{sql}
query entry to probe the data. This would be an excellent
opportunity for some documentation about the table
structure. Documenting that structure would pose a
serious challenge because the table structure would probably
change going to the next version of the application.
It might also be easy to mistake the meaning of data
within the tables in order to draw erroneous conclusions
about simulation results. Even so, the database format
is sensible and a tantalizing resource. It would be
great to have available within the \textsc{sc} version
of the code.


\subsection{Input Files}
Scenario files and herd files for \naadsm come in
two formats, one for the \gui and one for the \nsc version.
The \nsc files are in \textsc{utf--16} instead of \textsc{utf--8}.
Their \textsc{xml} format includes a document type definition
(\textsc{dtd}), but the namespace in the scenario files
isn't actually at the specified \textsc{url} and the 
\textsc{unit} element isn't in any referenced namespace.
As a result of these minor vagaries, the input files
can't be read by standard parsers without editing.
None of this is a big deal, except that modification
of the \textsc{xml} directly is an excellent way to
create code which interacts with \naadsm as a dynamical
model beneath statistical software.


\section{Specific Recommendations to Reduce Defects}
\subsection{Developer Documentation}

Developer documentation is a guideline for contributors
to the code. It covers high-level structure, best practices,
and formatting guidelines. All of these are associated with
improved code quality. This documentation doesn't have to be
as polished. It may include images of whiteboards or recorded
talks.

Within \naadsm, developer documentation might cover practices
for sampling distributions, as discussed below. It might cover
the determinism of how a model interacts with other models
when two models predict conflicting state changes. Both of these
are important for correctness.


\subsection{Testing Stochastic Simulations}

\naadsm uses extensive unit testing. Unit tests mimic
a user interacting with a user interface in order to
answer whether a deliverable feature of an application
works according to expectations. It is one of the types
of tests for software.

Because the mathematical model underlying \naadsm's
calculations is stochastic, how the simulation responds
will depend both on the input scenario and initialization
of its random number generation. The current testing scheme
relies on the Gnu DejaGnu testing framework, set up to
expect a known set of responses. It fixes all random number
generators to respond with the same value for every query.
This is a way to test some basic behaviors of the interface
but doesn't test any behavior that is stochastic.

Performing a stochastic test on a simple scenario shows
that the rate of change of infection predicted by the
simulation doesn't match the input parameters from field
observation. The scenario is a single unit with a latent
infection of \hpai. The unit progresses to a clinically-infectious
state and then a recovered state. When the scenario is
run thousands of times, the distribution of
times for these states should match the input
parameters from the \hpai scenario.

The input parameters for disease states in the \hpai
scenario are shown in Table~\ref{table:hpaiparams}.
The average time in each state doesn't match
the average time of the inputs, so something is off.


We can see from lower-level verification tests how
sampling for transition times behaves.
Modern practice eschews these lower-level
tests for general software because
the tests, themselves, restrict the programmer's ability to refactor
software. However, scientific software often follows a clear
mathematical structure which permits testing clear atomic units,
such as statistical distributions and, in this case, the core
functionality of sampling a discrete time step.

We see that the sampling is biased by the change in domain
size. The fix is a multiplication by two.


\subsection{Equations}
There were three instances, during our work, where
transferring equations from specifications to code
created difficulty either for correctness or for understanding
what the code means.

One of the input distributions of the \hpai scenario is
a gamma distribution with two parameters. Those parameters
are labeled ``alpha'' and ``beta.'' The gamma distribution
can be specified two different ways, either with a
shape and a scale or a shape and a rate. The ``alpha'' and
``beta'' names usually refer to a shape and a rate but here
refer to a shape and a scale. The authors' sense of ``usually''
comes from a few statistical textbooks and Wikipedia.
\naadsm includes a manual, not the specification but a user
guide, that is clear which distribution they intend, so they
did the right thing, but it's still very simple to mistake
input parameters.

The \naadsm specification for indirect contact (movement of
vehicles among units) uses a probability distribution to
determine at what radius to find the truck's destination.
Choosing a uniform distribution for truck movement appears
to be a MaxEnt choice. It appears to state that trucks
will travel within some cutoff radius without bias to
distance. However, the code doesn't normalize the radius
by $1/r$, meaning a uniform distribution strongly favors
trucks which travel short distances. The specification is
again correct but easily misinterpreted.

Lastly, there the airborne spread model models infection
of neighboring units by wind or other causes. It
makes sense that small units might be less infectious
to their neighbors than large units. A ``size factor''
accounts for this. The size factor, as stated by
an equation in the specifications, doesn't match
the internal calculation of the size factor.
In this case, the summation done for the ``size factor'' uses
bins shifted to the left, resulting in values that are
averaged with the neighboring bin.

% Size factor more troubling b/c it's included so that results
% look right but has no justification. It's inherently qualitative.
% Should be subject to model selection.

It is oddly, persistently, difficult to ensure that
a mathematical assignment within the code corresponds to
that intended. There are many ways this can go wrong, and we
have examples at each step.
\begin{enumerate}
    \item An equation in the specification can become separated
          from what behavior that equation was supposed to express.
          (Where is the size factor from?)
    \item A clearly-stated equation can have more than one
          mathematical interpretation. (Which gamma distribution do you mean?)
    \item The code can have a defect in implementation of an equation,
          which can be as subtle as having an arcsin with a different
          domain.
          (The size factor code is more complicated than the specification.)
    \item A correctly coded equation can be incorporated mistakenly
          in a model. (The gamma distribution, even though correct, is sampled incorrectly.)
\end{enumerate}
Whether an encoding reflects an author's intent is as querulous
in computational simulation as it is in translation of poetry.
There are some kinds of formal verification that can work
for mathematical programming, but we mostly rely on two
techniques, documentation and testing.

Forget for the moment connotations of documentation and think
instead about narrative, about telling a story that explains
what causes the next step. For instance, it's likely that the
$1/r$ normalization of the airborne model is intentional, but
we don't understand the reasoning. Strength of this narrative
depends on whether the explanation can precede what it explains
for the reader and whether each step can be understood with
the information provided thus far.
There's no good answer for how to do this, so let's look at
some methods.

\emph{Literate programming\/} is a technique where one writes
a document in which are embedded snippets of code. The document
would, in this case, explain the structure of the simulation
and reasoning for each model, pairing implementation in code
right after the need for that code is explained.
Compiling the code then extracts the snippets from the document,
weaves them together, and compiles as usual.
For a small, mathematical problem, writing the code and mathematics
together, start to finish, in this way, creates a powerful
argument. Refactoring code becomes more difficult, as
does any kind of boilerplate. The R programming language
has literate programming tools builtin.

The same idea, in reverse, is to write a specification
document which draws quotations of code from the repository.
Doxygen and Python's Sphinx tools work this way. The editor sucks a few
lines into a code box while compiling a \textsc{pdf}.

Equations in code are often derived from other equations.
Symbolic mathematics tools can write code for you.
Mathematica, Python's sympy, Matlab, and more
all generate code. There is always a possibility these
tools might be in error, but they are much more likely
to get the overall structure of a large polynomial correct.

The Systems Biology Markup Language\cite{hucka2003systems}
encodes equations using MathML, which specifies transcendental
equations, such as the gamma distribution, using 
OpenMath Content Directories. This makes equation specification
in \textsc{xml} both more general and less subject to
variation in what a definition might mean.
Translation among MathML, LaTeX, and various computer languages
has improved greatly in the past few years.

Testing of code, as described in a section above, is quantitative.
Qualitative results can also be meaningful. We could run
the simulation, measure the rate of transitions from one
state to another, and graph it. That graph should match the
graphs provided in the article defining the distributions.


\subsection{Qualitative Calculation}
Let's walk through two examples in \naadsm and then
discuss how they are qualitative.

The ``size factor'' in the airborne kernel is complicated.
It gives proportional results. There is no association
with a biological behavior.

Indirect contact interacts differently with quarantine
than it does with zones. Kernels matter when
they are quantitative, else why do this?\cite{buhnerkempe2013national}


\subsection{Model Definition}
The most powerful improvement for correctness would be
a clear definition of the mathematical model upon which
the \naadsm simulation is based.

Landscape, biological processes, initial conditions,
provenance, runtime behavior, are all different
and shouldn't be mixed in the files.

\section{Conclusion}
Credo. Simulation starts with a mathematical model.


(Lastly, what if \naadsm were run in an emergency? How would
that affect requirements on building it?)

\bibliographystyle{plain}
\bibliography{softeng}
\end{document}


